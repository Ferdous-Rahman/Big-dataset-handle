# -*- coding: utf-8 -*-
"""AQI_prediction_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dOXbe2UlKtGxC9DdYzMwiIirEITl9py8

# Data process
"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from google.colab import files  # For download link in Colab

# === 1. Load dataset ===
file_path = "/content/Air Quality Data_Narsingdi CAMS_2020-2024.csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)
print("✅ Dataset loaded. Shape:", df.shape)

# === 2. Drop fully empty columns ===
df.dropna(axis=1, how='all', inplace=True)
print("✅ Dropped fully empty columns. Shape:", df.shape)

# === 3. Convert all columns to numeric where possible (except Date & Time) ===
for col in df.columns:
    if col not in ['Date', 'Time']:  # keep Date & Time raw for datetime parsing
        df[col] = pd.to_numeric(df[col], errors='coerce')

# === 4. Create DateTime features if Date and Time columns exist ===
if 'Date' in df.columns and 'Time' in df.columns:
    df['DateTime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')
    df['hour'] = df['DateTime'].dt.hour
    df['dayofweek'] = df['DateTime'].dt.dayofweek
    print("✅ DateTime features added.")

# === 5. Keep only numeric columns for KNN imputation (exclude Date & Time) ===
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [col for col in numeric_cols if col not in ['Date', 'Time']]

# Drop numeric columns that are entirely NaN (KNNImputer cannot handle them)
numeric_cols = [col for col in numeric_cols if not df[col].isna().all()]

print("✅ Final numeric columns for imputation:", numeric_cols)

# === 6. KNN Imputation ===
imputer = KNNImputer(n_neighbors=5, weights="distance")
imputed_array = imputer.fit_transform(df[numeric_cols])

# Replace numeric columns with imputed values
df[numeric_cols] = pd.DataFrame(imputed_array, columns=numeric_cols, index=df.index)
print("✅ KNN imputation complete!")

# === 7. Drop unwanted 'Unnamed' columns ===
unnamed_cols = [c for c in df.columns if "Unnamed" in c]
df.drop(columns=unnamed_cols, inplace=True)
print(f"✅ Dropped unwanted columns: {unnamed_cols}")

# === 8. Save refined dataset ===
refined_file = "/content/Air_Quality_Refined.csv"
df.to_csv(refined_file, index=False)
print(f"✅ Refined dataset saved at: {refined_file}")

# === 9. Provide download link in Colab ===
files.download(refined_file)

"""# refined and added AQI"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from google.colab import files

# === 1. Load dataset ===
file_path = "/content/Air_Quality_Refined.csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)
print("✅ Dataset loaded. Shape:", df.shape)

# === 2. Drop fully empty columns ===
df.dropna(axis=1, how='all', inplace=True)
print("✅ Dropped fully empty columns. Shape:", df.shape)

# === 3. Convert to numeric where possible (except Date & Time) ===
for col in df.columns:
    if col not in ['Date', 'Time']:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# === 4. Create DateTime column and daily date ===
if 'Date' in df.columns and 'Time' in df.columns:
    df['DateTime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')
    df['DateOnly'] = df['DateTime'].dt.date

# === 5. Select numeric cols for KNN imputation (exclude Date/Time) ===
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [col for col in numeric_cols if col not in ['Date', 'Time']]

# Drop all-NaN numeric columns
numeric_cols = [col for col in numeric_cols if not df[col].isna().all()]

print("✅ Final numeric columns for imputation:", numeric_cols)

# === 6. KNN Imputation ===
imputer = KNNImputer(n_neighbors=5, weights="distance")
imputed_array = imputer.fit_transform(df[numeric_cols])
df[numeric_cols] = pd.DataFrame(imputed_array, columns=numeric_cols, index=df.index)
print("✅ KNN imputation complete!")

# === 7. Drop unwanted columns ===
drop_cols = ['Ratio', 'DateTime', 'hour', 'dayofweek']
df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors='ignore')
print(f"✅ Dropped unwanted columns: {drop_cols}")

# === 8. AQI Calculation (Row-level) ===
# Define AQI breakpoints (India CPCB standard)
aqi_breakpoints = {
    'PM2.5': [(0, 30, 0, 50), (31, 60, 51, 100), (61, 90, 101, 200),
              (91, 120, 201, 300), (121, 250, 301, 400), (251, 500, 401, 500)],
    'PM10':  [(0, 50, 0, 50), (51, 100, 51, 100), (101, 250, 101, 200),
              (251, 350, 201, 300), (351, 430, 301, 400), (431, 600, 401, 500)],
    'NO2':   [(0, 40, 0, 50), (41, 80, 51, 100), (81, 180, 101, 200),
              (181, 280, 201, 300), (281, 400, 301, 400), (401, 1000, 401, 500)],
    'SO2':   [(0, 40, 0, 50), (41, 80, 51, 100), (81, 380, 101, 200),
              (381, 800, 201, 300), (801, 1600, 301, 400), (1601, 2000, 401, 500)],
    'CO':    [(0, 1, 0, 50), (1.1, 2, 51, 100), (2.1, 10, 101, 200),
              (10.1, 17, 201, 300), (17.1, 34, 301, 400), (34.1, 50, 401, 500)],
    'O3':    [(0, 50, 0, 50), (51, 100, 51, 100), (101, 168, 101, 200),
              (169, 208, 201, 300), (209, 748, 301, 400), (749, 1000, 401, 500)]
}

def calc_sub_index(pollutant, value):
    if pd.isna(value):
        return np.nan
    for (Clow, Chigh, Ilow, Ihigh) in aqi_breakpoints[pollutant]:
        if Clow <= value <= Chigh:
            return ((Ihigh - Ilow) / (Chigh - Clow)) * (value - Clow) + Ilow
    return np.nan

# Calculate AQI row by row
aqi_values = []
for _, row in df.iterrows():
    sub_indices = []
    for pollutant in aqi_breakpoints.keys():
        if pollutant in df.columns:
            si = calc_sub_index(pollutant, row[pollutant])
            if not pd.isna(si):
                sub_indices.append(si)
    aqi_val = max(sub_indices) if sub_indices else np.nan
    aqi_values.append(aqi_val)

df['AQI'] = aqi_values
print("✅ Row-level AQI column calculated.")

# === 9. Save refined dataset ===
refined_file = "/content/Air_Quality_Refined_with_Rowwise_AQI.csv"
df.to_csv(refined_file, index=False)
print(f"✅ Refined dataset with AQI saved at: {refined_file}")

# === 10. Provide download link in Colab ===
files.download(refined_file)

"""# MODELS ML based

# SVR + RF
"""

# ================================
# Install dependencies (if needed)
# ================================
!pip install scikit-learn pandas numpy matplotlib --quiet

# ================================
# Import libraries
# ================================
import pandas as pd, numpy as np, matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_recall_fscore_support,
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    precision_score, recall_score, f1_score
)
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor

# ================================
# STEP 1: Load dataset
# ================================
df = pd.read_csv("/content/BD_AQI_2020_2024.csv")

print(" Data Loaded:", df.shape)
print(df.head())

# ================================
# STEP 2: Preprocess
# ================================
df.drop(['Date', 'Time'], axis=1, inplace=True, errors='ignore')
df.dropna(inplace=True)

X = df.drop("AQI", axis=1)
y = df["AQI"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ================================
# STEP 3: Train SVR
# ================================
svr = SVR(kernel='rbf', C=100, epsilon=0.1)
svr.fit(X_train, y_train)

svr_pred = svr.predict(X_test)

# Compute residuals
residuals_train = y_train - svr.predict(X_train)
residuals_test = y_test - svr_pred

# ================================
# STEP 4: Train Random Forest on residuals
# ================================
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, residuals_train)

# Correct SVR predictions
rf_correction = rf.predict(X_test)
hybrid_pred = svr_pred + rf_correction

# ================================
# STEP 5: Regression Evaluation
# ================================
rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))
mae = mean_absolute_error(y_test, hybrid_pred)
r2 = r2_score(y_test, hybrid_pred)

print(" Hybrid SVR + RF (Residual Correction) Results")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"R² Score: {r2:.4f}")

# ================================
# STEP 6: Classification Metrics
# ================================
# AQI category bins and labels
bins = [-np.inf, 50, 100, 150, 200, 300, np.inf]
labels = ['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Hazardous']

# Convert continuous values to categories
y_test_cls = pd.cut(y_test.values, bins=bins, labels=labels, right=True).astype(str)
y_pred_cls = pd.cut(hybrid_pred,  bins=bins, labels=labels, right=True).astype(str)

# Accuracy
acc = accuracy_score(y_test_cls, y_pred_cls)

# Macro & weighted averages
prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(
    y_test_cls, y_pred_cls, labels=labels, average='macro', zero_division=0
)
prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(
    y_test_cls, y_pred_cls, labels=labels, average='weighted', zero_division=0
)

print("\n Classification Metrics on AQI Categories")
print(f"Accuracy: {acc:.3f}")
print(f"Precision (macro): {prec_macro:.3f} | Recall (macro): {rec_macro:.3f} | F1 (macro): {f1_macro:.3f}")
print(f"Precision (weighted): {prec_weighted:.3f} | Recall (weighted): {rec_weighted:.3f} | F1 (weighted): {f1_weighted:.3f}")

print("\nPer-Class Report:")
print(classification_report(y_test_cls, y_pred_cls, labels=labels, digits=3, zero_division=0))

# Confusion Matrix
cm = confusion_matrix(y_test_cls, y_pred_cls, labels=labels)
fig, ax = plt.subplots(figsize=(8, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(ax=ax, xticks_rotation=45, values_format='d')
ax.set_title("Confusion Matrix – AQI Categories (Hybrid SVR+RF)")
plt.tight_layout()
plt.show()

# ================================
# STEP 7: Binary Threshold Metrics
# ================================
THRESHOLD = 150  # AQI > 150 = Unhealthy or worse
y_test_bin = (y_test.values > THRESHOLD).astype(int)
y_pred_bin = (hybrid_pred > THRESHOLD).astype(int)

acc_bin = accuracy_score(y_test_bin, y_pred_bin)
prec_bin = precision_score(y_test_bin, y_pred_bin, zero_division=0)
rec_bin  = recall_score(y_test_bin, y_pred_bin, zero_division=0)
f1_bin   = f1_score(y_test_bin, y_pred_bin, zero_division=0)

print("\n Binary Metrics (Unhealthy-or-worse threshold > 150)")
print(f"Accuracy: {acc_bin:.3f} | Precision: {prec_bin:.3f} | Recall: {rec_bin:.3f} | F1: {f1_bin:.3f}")

# ================================
# STEP 8: Plot Predictions
# ================================
plt.figure(figsize=(12,6))
plt.plot(y_test.values[:100], label="Actual AQI")
plt.plot(svr_pred[:100], label="SVR Predicted AQI")
plt.plot(hybrid_pred[:100], label="Hybrid SVR+RF AQI")
plt.legend()
plt.title("Hybrid SVR + RF Residual Correction (First 100 samples)")
plt.xlabel("Time Steps")
plt.ylabel("AQI")
plt.grid(True)
plt.show()

"""#XGBoost + KNN"""

# ================================
# Install dependencies (if needed)
# ================================
!pip install scikit-learn xgboost scikit-optimize pandas numpy matplotlib --quiet

# ================================
# Import libraries
# ================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_recall_fscore_support,
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    precision_score, recall_score, f1_score
)
from xgboost import XGBRegressor
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# ================================
# STEP 1: Load dataset
# ================================
df = pd.read_csv("/content/BD_AQI_2020_2024.csv")
df.drop(['Date', 'Time'], axis=1, inplace=True, errors='ignore')
df.dropna(inplace=True)

# Features & Target
X = df.drop("AQI", axis=1)
y = df["AQI"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ================================
# STEP 2: Define base model
# ================================
opt_model = XGBRegressor(objective='reg:squarederror', random_state=42)

# Define search space
search_space = {
    'n_estimators': Integer(50, 300),
    'max_depth': Integer(3, 10),
    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
    'subsample': Real(0.6, 1.0),
    'colsample_bytree': Real(0.6, 1.0)
}

# ================================
# STEP 3: Bayesian Optimization
# ================================
opt = BayesSearchCV(
    opt_model,
    search_spaces=search_space,
    n_iter=32,
    cv=3,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1
)

opt.fit(X_train, y_train)

# Best model
best_model = opt.best_estimator_

# Predictions
y_pred = best_model.predict(X_test)

# ================================
# STEP 4: Regression Evaluation
# ================================
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Best Parameters:", opt.best_params_)
print("\n Regression Metrics")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"R²: {r2:.4f}")

# ================================
# STEP 5: Classification Metrics
# ================================
# AQI category bins and labels
bins = [-np.inf, 50, 100, 150, 200, 300, np.inf]
labels = ['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Hazardous']

y_test_cls = pd.cut(y_test.values, bins=bins, labels=labels, right=True).astype(str)
y_pred_cls = pd.cut(y_pred,       bins=bins, labels=labels, right=True).astype(str)

# Overall accuracy
acc = accuracy_score(y_test_cls, y_pred_cls)

# Macro & weighted averages
prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(
    y_test_cls, y_pred_cls, labels=labels, average='macro', zero_division=0
)
prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(
    y_test_cls, y_pred_cls, labels=labels, average='weighted', zero_division=0
)

print("\n Classification Metrics on AQI Categories")
print(f"Accuracy: {acc:.3f}")
print(f"Precision (macro): {prec_macro:.3f} | Recall (macro): {rec_macro:.3f} | F1 (macro): {f1_macro:.3f}")
print(f"Precision (weighted): {prec_weighted:.3f} | Recall (weighted): {rec_weighted:.3f} | F1 (weighted): {f1_weighted:.3f}")

print("\nPer-Class Report:")
print(classification_report(y_test_cls, y_pred_cls, labels=labels, digits=3, zero_division=0))

# Confusion Matrix
cm = confusion_matrix(y_test_cls, y_pred_cls, labels=labels)
fig, ax = plt.subplots(figsize=(8, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(ax=ax, xticks_rotation=45, values_format='d')
ax.set_title("Confusion Matrix – AQI Categories (XGBoost + Bayesian Opt)")
plt.tight_layout()
plt.show()

# ================================
# STEP 6: Binary Threshold Metrics
# ================================
THRESHOLD = 150  # AQI > 150 = Unhealthy or worse
y_test_bin = (y_test.values > THRESHOLD).astype(int)
y_pred_bin = (y_pred > THRESHOLD).astype(int)

acc_bin = accuracy_score(y_test_bin, y_pred_bin)
prec_bin = precision_score(y_test_bin, y_pred_bin, zero_division=0)
rec_bin  = recall_score(y_test_bin, y_pred_bin, zero_division=0)
f1_bin   = f1_score(y_test_bin, y_pred_bin, zero_division=0)

print("\n Binary Metrics (Unhealthy-or-worse threshold > 150)")
print(f"Accuracy: {acc_bin:.3f} | Precision: {prec_bin:.3f} | Recall: {rec_bin:.3f} | F1: {f1_bin:.3f}")

# ================================
# STEP 7: Plot Predictions
# ================================
plt.figure(figsize=(12,6))
plt.plot(y_test.values[:100], label="Actual AQI")
plt.plot(y_pred[:100], label="Predicted AQI")
plt.legend()
plt.title("XGBoost + Bayesian Optimization: Actual vs Predicted AQI (First 100 samples)")
plt.xlabel("Time Steps")
plt.ylabel("AQI")
plt.grid(True)
plt.show()

"""# RF + XGB + LightGBM"""

# ================================
# Install dependencies (if needed)
# ================================
!pip install xgboost lightgbm scikit-learn pandas numpy matplotlib --quiet

# ================================
# Import libraries
# ================================
import pandas as pd, numpy as np, matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_recall_fscore_support,
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    precision_score, recall_score, f1_score
)
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.linear_model import Ridge

# ================================
# STEP 1: Load dataset
# ================================
df = pd.read_csv("/content/BD_AQI_2020_2024.csv")

print(" Data Loaded:", df.shape)
print(df.head())

# ================================
# STEP 2: Preprocess
# ================================
df.drop(['Date', 'Time'], axis=1, inplace=True, errors='ignore')
df.dropna(inplace=True)

X = df.drop("AQI", axis=1)
y = df["AQI"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ================================
# STEP 3: Define base models
# ================================
rf = RandomForestRegressor(n_estimators=200, random_state=42)
xgb = XGBRegressor(
    n_estimators=200, learning_rate=0.1, max_depth=6,
    subsample=0.8, colsample_bytree=0.8, random_state=42
)
lgb = LGBMRegressor(
    n_estimators=200, learning_rate=0.1, max_depth=-1,
    subsample=0.8, colsample_bytree=0.8, random_state=42
)

# ================================
# STEP 4: Build Stacking Regressor
# ================================
stacked_model = StackingRegressor(
    estimators=[('rf', rf), ('xgb', xgb), ('lgb', lgb)],
    final_estimator=Ridge(alpha=1.0)
)

# ================================
# STEP 5: Train
# ================================
stacked_model.fit(X_train, y_train)

# ================================
# STEP 6: Regression Evaluation
# ================================
y_pred = stacked_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n Stacked Ensemble (RF + XGB + LGBM) Results")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"R² Score: {r2:.4f}")

# ================================
# STEP 6b: Classification Metrics
# ================================
# AQI category bins and labels (standard US EPA scale)
bins = [-np.inf, 50, 100, 150, 200, 300, np.inf]
labels = ['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Hazardous']

# Map continuous AQI values to categories
y_test_cls = pd.cut(y_test.values, bins=bins, labels=labels, right=True).astype(str)
y_pred_cls = pd.cut(y_pred,       bins=bins, labels=labels, right=True).astype(str)

# Overall accuracy
acc = accuracy_score(y_test_cls, y_pred_cls)

# Macro and weighted averages
prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(
    y_test_cls, y_pred_cls, labels=labels, average='macro', zero_division=0
)
prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(
    y_test_cls, y_pred_cls, labels=labels, average='weighted', zero_division=0
)

print("\n Classification Metrics on AQI Categories")
print(f"Accuracy: {acc:.3f}")
print(f"Precision (macro): {prec_macro:.3f} | Recall (macro): {rec_macro:.3f} | F1 (macro): {f1_macro:.3f}")
print(f"Precision (weighted): {prec_weighted:.3f} | Recall (weighted): {rec_weighted:.3f} | F1 (weighted): {f1_weighted:.3f}")

print("\nPer-Class Report:")
print(classification_report(y_test_cls, y_pred_cls, labels=labels, digits=3, zero_division=0))

# Confusion Matrix
cm = confusion_matrix(y_test_cls, y_pred_cls, labels=labels)
fig, ax = plt.subplots(figsize=(8, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(ax=ax, xticks_rotation=45, values_format='d')
ax.set_title("Confusion Matrix – AQI Categories")
plt.tight_layout()
plt.show()

# ================================
# STEP 6c: Binary Threshold Metrics (Optional)
# ================================
THRESHOLD = 150  # AQI > 150 means Unhealthy or worse
y_test_bin = (y_test.values > THRESHOLD).astype(int)
y_pred_bin = (y_pred > THRESHOLD).astype(int)

acc_bin = accuracy_score(y_test_bin, y_pred_bin)
prec_bin = precision_score(y_test_bin, y_pred_bin, zero_division=0)
rec_bin  = recall_score(y_test_bin, y_pred_bin, zero_division=0)
f1_bin   = f1_score(y_test_bin, y_pred_bin, zero_division=0)

print("\n Binary Metrics (Unhealthy-or-worse threshold > 150)")
print(f"Accuracy: {acc_bin:.3f} | Precision: {prec_bin:.3f} | Recall: {rec_bin:.3f} | F1: {f1_bin:.3f}")

# ================================
# STEP 7: Plot Predictions
# ================================
plt.figure(figsize=(12,6))
plt.plot(y_test.values[:100], label="Actual AQI")
plt.plot(y_pred[:100], label="Predicted AQI")
plt.legend()
plt.title("Actual vs Predicted AQI (Stacked Ensemble, First 100 samples)")
plt.xlabel("Time Steps")
plt.ylabel("AQI")
plt.grid(True)
plt.show()

"""# Deep learning models (ANN, CNN, LSTM)"""

# ======================================
# Install dependencies
# ======================================
!pip install scikit-learn pandas numpy matplotlib tensorflow --quiet

# ======================================
# Import libraries
# ======================================
import pandas as pd, numpy as np, matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_recall_fscore_support,
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    precision_score, recall_score, f1_score
)
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Input, LSTM

# ======================================
# STEP 1: Load dataset
# ======================================
df = pd.read_csv("/content/BD_AQI_2020_2024.csv")
df.drop(['Date', 'Time'], axis=1, inplace=True, errors='ignore')
df.dropna(inplace=True)

X = df.drop("AQI", axis=1)
y = df["AQI"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ======================================
# Helper: Evaluate regression + classification
# ======================================
def evaluate_all(y_true, y_pred, model_name):
    # Regression metrics
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"\n {model_name} Regression Metrics")
    print(f"RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.4f}")

    # Classification bins
    bins = [-np.inf, 50, 100, 150, 200, 300, np.inf]
    labels = ['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Hazardous']
    y_true_cls = pd.cut(y_true, bins=bins, labels=labels, right=True).astype(str)
    y_pred_cls = pd.cut(y_pred, bins=bins, labels=labels, right=True).astype(str)

    # Classification metrics
    acc = accuracy_score(y_true_cls, y_pred_cls)
    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(
        y_true_cls, y_pred_cls, labels=labels, average='macro', zero_division=0
    )
    print(f"\n {model_name} Classification Metrics")
    print(f"Accuracy: {acc:.3f}")
    print(f"Precision (macro): {prec_macro:.3f} | Recall (macro): {rec_macro:.3f} | F1 (macro): {f1_macro:.3f}")
    print(classification_report(y_true_cls, y_pred_cls, labels=labels, digits=3, zero_division=0))

    # Confusion matrix
    cm = confusion_matrix(y_true_cls, y_pred_cls, labels=labels)
    fig, ax = plt.subplots(figsize=(8, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(ax=ax, xticks_rotation=45, values_format='d')
    ax.set_title(f"Confusion Matrix – {model_name}")
    plt.tight_layout()
    plt.show()

# ======================================
# STEP 2: ANN Model
# ======================================
ann = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(1)
])
ann.compile(optimizer='adam', loss='mse', metrics=['mae'])
ann.fit(X_train, y_train, validation_split=0.2, epochs=30, batch_size=32, verbose=1)

ann_pred = ann.predict(X_test).flatten()
evaluate_all(y_test, ann_pred, "ANN")

# ======================================
# STEP 3: CNN Model (1D)
# ======================================
X_train_cnn = np.expand_dims(X_train, axis=2)
X_test_cnn  = np.expand_dims(X_test, axis=2)

inp = Input(shape=(X_train_cnn.shape[1], 1))
x = Conv1D(64, 3, activation='relu')(inp)
x = MaxPooling1D(2)(x)
x = Conv1D(32, 3, activation='relu')(x)
x = Flatten()(x)
x = Dense(64, activation='relu')(x)
out = Dense(1)(x)

cnn = Model(inp, out)
cnn.compile(optimizer='adam', loss='mse', metrics=['mae'])
cnn.fit(X_train_cnn, y_train, validation_split=0.2, epochs=30, batch_size=32, verbose=1)

cnn_pred = cnn.predict(X_test_cnn).flatten()
evaluate_all(y_test, cnn_pred, "CNN")

# ======================================
# STEP 4: LSTM Model
# ======================================
X_train_lstm = np.expand_dims(X_train, axis=1)
X_test_lstm  = np.expand_dims(X_test, axis=1)

lstm = Sequential([
    LSTM(64, activation='tanh', return_sequences=False, input_shape=(1, X_train.shape[1])),
    Dense(32, activation='relu'),
    Dense(1)
])
lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])
lstm.fit(X_train_lstm, y_train, validation_split=0.2, epochs=30, batch_size=32, verbose=1)

lstm_pred = lstm.predict(X_test_lstm).flatten()
evaluate_all(y_test, lstm_pred, "LSTM")

# ======================================
# STEP 5: Plot Actual vs Predicted (ANN example)
# ======================================
plt.figure(figsize=(12,6))
plt.plot(y_test.values[:100], label="Actual AQI")
plt.plot(ann_pred[:100], label="ANN Predicted AQI")
plt.plot(cnn_pred[:100], label="CNN Predicted AQI")
plt.plot(lstm_pred[:100], label="LSTM Predicted AQI")
plt.legend()
plt.title("Deep Learning Models: Actual vs Predicted AQI (First 100 samples)")
plt.xlabel("Time Steps")
plt.ylabel("AQI")
plt.grid(True)
plt.show()

"""# (RF + XGB + LGBM) and evaluation per pollutant"""

# ================================
# Install dependencies
# ================================
!pip install scikit-learn xgboost lightgbm pandas numpy matplotlib --quiet

# ================================
# Import libraries
# ================================
import pandas as pd, numpy as np, matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.multioutput import MultiOutputRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.linear_model import Ridge

# ================================
# STEP 1: Load dataset
# ================================
df = pd.read_csv("/content/BD_AQI_2020_2024.csv")
print(" Data Loaded:", df.shape)
print(df.head())

# ================================
# STEP 2: Preprocess
# ================================
df.drop(['Date', 'Time'], axis=1, inplace=True, errors='ignore')
df.dropna(inplace=True)

# Features = all except pollutants
X = df.drop(["PM2.5","PM10","NO2","SO2","CO","O3"], axis=1)
# Targets = pollutants
y = df[["PM2.5","PM10","NO2","SO2","CO","O3"]]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ================================
# STEP 3: Define base regressors
# ================================
rf = RandomForestRegressor(n_estimators=200, random_state=42)
xgb = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6,
                   subsample=0.8, colsample_bytree=0.8, random_state=42)
lgb = LGBMRegressor(n_estimators=200, learning_rate=0.1,
                    subsample=0.8, colsample_bytree=0.8, random_state=42)

stacked = StackingRegressor(
    estimators=[('rf', rf), ('xgb', xgb), ('lgb', lgb)],
    final_estimator=Ridge(alpha=1.0)
)

# Multi-output wrapper
multi_model = MultiOutputRegressor(stacked)

# ================================
# STEP 4: Train
# ================================
multi_model.fit(X_train, y_train)

# ================================
# STEP 5: Predict
# ================================
y_pred = multi_model.predict(X_test)
y_pred = pd.DataFrame(y_pred, columns=y_test.columns, index=y_test.index)

# ================================
# STEP 6: Evaluate per pollutant
# ================================
results = []
for pol in y_test.columns:
    rmse = np.sqrt(mean_squared_error(y_test[pol], y_pred[pol]))
    mae = mean_absolute_error(y_test[pol], y_pred[pol])
    r2 = r2_score(y_test[pol], y_pred[pol])
    results.append([pol, rmse, mae, r2])

res_df = pd.DataFrame(results, columns=["Pollutant","RMSE","MAE","R²"])
print("\nPer-Pollutant Regression Metrics (Multi-output RF+XGB+LGBM):")
print(res_df)

# ================================
# STEP 7: Save results
# ================================
res_df.to_csv("/content/per_pollutant_regression_multioutput.csv", index=False)
print("\n✅ Saved per-pollutant regression metrics to per_pollutant_regression_multioutput.csv")

# ================================
# STEP 8: Example plots
# ================================
for pol in y_test.columns:
    plt.figure(figsize=(10,4))
    plt.plot(y_test[pol].values[:100], label=f"Actual {pol}")
    plt.plot(y_pred[pol].values[:100], label=f"Predicted {pol}")
    plt.legend()
    plt.title(f"Actual vs Predicted {pol} (First 100 samples)")
    plt.xlabel("Time Steps")
    plt.ylabel(pol)
    plt.grid(True)
    plt.show()